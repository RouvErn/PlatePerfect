{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.21.3 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow)\n",
      "  Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from tensorflow) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "Using cached h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Using cached google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, ml-dtypes, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, astunparse, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.28.1 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.62.0 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.1 wheel-0.42.0 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch)\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/chris_nill/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.2.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "Successfully installed mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.2.1 torchaudio-2.2.1 torchvision-0.17.1 triton-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.patch_embeddings.projection.weight\n",
      "embeddings.patch_embeddings.projection.bias\n",
      "embeddings.norm.weight\n",
      "embeddings.norm.bias\n",
      "encoder.layers.0.blocks.0.layernorm_before.weight\n",
      "encoder.layers.0.blocks.0.layernorm_before.bias\n",
      "encoder.layers.0.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.0.blocks.0.attention.self.query.weight\n",
      "encoder.layers.0.blocks.0.attention.self.query.bias\n",
      "encoder.layers.0.blocks.0.attention.self.key.weight\n",
      "encoder.layers.0.blocks.0.attention.self.key.bias\n",
      "encoder.layers.0.blocks.0.attention.self.value.weight\n",
      "encoder.layers.0.blocks.0.attention.self.value.bias\n",
      "encoder.layers.0.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.0.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.0.blocks.0.layernorm_after.weight\n",
      "encoder.layers.0.blocks.0.layernorm_after.bias\n",
      "encoder.layers.0.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.0.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.0.blocks.0.output.dense.weight\n",
      "encoder.layers.0.blocks.0.output.dense.bias\n",
      "encoder.layers.0.blocks.1.layernorm_before.weight\n",
      "encoder.layers.0.blocks.1.layernorm_before.bias\n",
      "encoder.layers.0.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.0.blocks.1.attention.self.query.weight\n",
      "encoder.layers.0.blocks.1.attention.self.query.bias\n",
      "encoder.layers.0.blocks.1.attention.self.key.weight\n",
      "encoder.layers.0.blocks.1.attention.self.key.bias\n",
      "encoder.layers.0.blocks.1.attention.self.value.weight\n",
      "encoder.layers.0.blocks.1.attention.self.value.bias\n",
      "encoder.layers.0.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.0.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.0.blocks.1.layernorm_after.weight\n",
      "encoder.layers.0.blocks.1.layernorm_after.bias\n",
      "encoder.layers.0.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.0.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.0.blocks.1.output.dense.weight\n",
      "encoder.layers.0.blocks.1.output.dense.bias\n",
      "encoder.layers.0.downsample.reduction.weight\n",
      "encoder.layers.0.downsample.norm.weight\n",
      "encoder.layers.0.downsample.norm.bias\n",
      "encoder.layers.1.blocks.0.layernorm_before.weight\n",
      "encoder.layers.1.blocks.0.layernorm_before.bias\n",
      "encoder.layers.1.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.1.blocks.0.attention.self.query.weight\n",
      "encoder.layers.1.blocks.0.attention.self.query.bias\n",
      "encoder.layers.1.blocks.0.attention.self.key.weight\n",
      "encoder.layers.1.blocks.0.attention.self.key.bias\n",
      "encoder.layers.1.blocks.0.attention.self.value.weight\n",
      "encoder.layers.1.blocks.0.attention.self.value.bias\n",
      "encoder.layers.1.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.1.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.1.blocks.0.layernorm_after.weight\n",
      "encoder.layers.1.blocks.0.layernorm_after.bias\n",
      "encoder.layers.1.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.1.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.1.blocks.0.output.dense.weight\n",
      "encoder.layers.1.blocks.0.output.dense.bias\n",
      "encoder.layers.1.blocks.1.layernorm_before.weight\n",
      "encoder.layers.1.blocks.1.layernorm_before.bias\n",
      "encoder.layers.1.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.1.blocks.1.attention.self.query.weight\n",
      "encoder.layers.1.blocks.1.attention.self.query.bias\n",
      "encoder.layers.1.blocks.1.attention.self.key.weight\n",
      "encoder.layers.1.blocks.1.attention.self.key.bias\n",
      "encoder.layers.1.blocks.1.attention.self.value.weight\n",
      "encoder.layers.1.blocks.1.attention.self.value.bias\n",
      "encoder.layers.1.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.1.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.1.blocks.1.layernorm_after.weight\n",
      "encoder.layers.1.blocks.1.layernorm_after.bias\n",
      "encoder.layers.1.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.1.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.1.blocks.1.output.dense.weight\n",
      "encoder.layers.1.blocks.1.output.dense.bias\n",
      "encoder.layers.1.downsample.reduction.weight\n",
      "encoder.layers.1.downsample.norm.weight\n",
      "encoder.layers.1.downsample.norm.bias\n",
      "encoder.layers.2.blocks.0.layernorm_before.weight\n",
      "encoder.layers.2.blocks.0.layernorm_before.bias\n",
      "encoder.layers.2.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.0.attention.self.query.weight\n",
      "encoder.layers.2.blocks.0.attention.self.query.bias\n",
      "encoder.layers.2.blocks.0.attention.self.key.weight\n",
      "encoder.layers.2.blocks.0.attention.self.key.bias\n",
      "encoder.layers.2.blocks.0.attention.self.value.weight\n",
      "encoder.layers.2.blocks.0.attention.self.value.bias\n",
      "encoder.layers.2.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.0.layernorm_after.weight\n",
      "encoder.layers.2.blocks.0.layernorm_after.bias\n",
      "encoder.layers.2.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.0.output.dense.weight\n",
      "encoder.layers.2.blocks.0.output.dense.bias\n",
      "encoder.layers.2.blocks.1.layernorm_before.weight\n",
      "encoder.layers.2.blocks.1.layernorm_before.bias\n",
      "encoder.layers.2.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.1.attention.self.query.weight\n",
      "encoder.layers.2.blocks.1.attention.self.query.bias\n",
      "encoder.layers.2.blocks.1.attention.self.key.weight\n",
      "encoder.layers.2.blocks.1.attention.self.key.bias\n",
      "encoder.layers.2.blocks.1.attention.self.value.weight\n",
      "encoder.layers.2.blocks.1.attention.self.value.bias\n",
      "encoder.layers.2.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.1.layernorm_after.weight\n",
      "encoder.layers.2.blocks.1.layernorm_after.bias\n",
      "encoder.layers.2.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.1.output.dense.weight\n",
      "encoder.layers.2.blocks.1.output.dense.bias\n",
      "encoder.layers.2.blocks.2.layernorm_before.weight\n",
      "encoder.layers.2.blocks.2.layernorm_before.bias\n",
      "encoder.layers.2.blocks.2.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.2.attention.self.query.weight\n",
      "encoder.layers.2.blocks.2.attention.self.query.bias\n",
      "encoder.layers.2.blocks.2.attention.self.key.weight\n",
      "encoder.layers.2.blocks.2.attention.self.key.bias\n",
      "encoder.layers.2.blocks.2.attention.self.value.weight\n",
      "encoder.layers.2.blocks.2.attention.self.value.bias\n",
      "encoder.layers.2.blocks.2.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.2.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.2.layernorm_after.weight\n",
      "encoder.layers.2.blocks.2.layernorm_after.bias\n",
      "encoder.layers.2.blocks.2.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.2.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.2.output.dense.weight\n",
      "encoder.layers.2.blocks.2.output.dense.bias\n",
      "encoder.layers.2.blocks.3.layernorm_before.weight\n",
      "encoder.layers.2.blocks.3.layernorm_before.bias\n",
      "encoder.layers.2.blocks.3.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.3.attention.self.query.weight\n",
      "encoder.layers.2.blocks.3.attention.self.query.bias\n",
      "encoder.layers.2.blocks.3.attention.self.key.weight\n",
      "encoder.layers.2.blocks.3.attention.self.key.bias\n",
      "encoder.layers.2.blocks.3.attention.self.value.weight\n",
      "encoder.layers.2.blocks.3.attention.self.value.bias\n",
      "encoder.layers.2.blocks.3.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.3.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.3.layernorm_after.weight\n",
      "encoder.layers.2.blocks.3.layernorm_after.bias\n",
      "encoder.layers.2.blocks.3.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.3.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.3.output.dense.weight\n",
      "encoder.layers.2.blocks.3.output.dense.bias\n",
      "encoder.layers.2.blocks.4.layernorm_before.weight\n",
      "encoder.layers.2.blocks.4.layernorm_before.bias\n",
      "encoder.layers.2.blocks.4.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.4.attention.self.query.weight\n",
      "encoder.layers.2.blocks.4.attention.self.query.bias\n",
      "encoder.layers.2.blocks.4.attention.self.key.weight\n",
      "encoder.layers.2.blocks.4.attention.self.key.bias\n",
      "encoder.layers.2.blocks.4.attention.self.value.weight\n",
      "encoder.layers.2.blocks.4.attention.self.value.bias\n",
      "encoder.layers.2.blocks.4.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.4.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.4.layernorm_after.weight\n",
      "encoder.layers.2.blocks.4.layernorm_after.bias\n",
      "encoder.layers.2.blocks.4.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.4.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.4.output.dense.weight\n",
      "encoder.layers.2.blocks.4.output.dense.bias\n",
      "encoder.layers.2.blocks.5.layernorm_before.weight\n",
      "encoder.layers.2.blocks.5.layernorm_before.bias\n",
      "encoder.layers.2.blocks.5.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.5.attention.self.query.weight\n",
      "encoder.layers.2.blocks.5.attention.self.query.bias\n",
      "encoder.layers.2.blocks.5.attention.self.key.weight\n",
      "encoder.layers.2.blocks.5.attention.self.key.bias\n",
      "encoder.layers.2.blocks.5.attention.self.value.weight\n",
      "encoder.layers.2.blocks.5.attention.self.value.bias\n",
      "encoder.layers.2.blocks.5.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.5.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.5.layernorm_after.weight\n",
      "encoder.layers.2.blocks.5.layernorm_after.bias\n",
      "encoder.layers.2.blocks.5.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.5.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.5.output.dense.weight\n",
      "encoder.layers.2.blocks.5.output.dense.bias\n",
      "encoder.layers.2.blocks.6.layernorm_before.weight\n",
      "encoder.layers.2.blocks.6.layernorm_before.bias\n",
      "encoder.layers.2.blocks.6.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.6.attention.self.query.weight\n",
      "encoder.layers.2.blocks.6.attention.self.query.bias\n",
      "encoder.layers.2.blocks.6.attention.self.key.weight\n",
      "encoder.layers.2.blocks.6.attention.self.key.bias\n",
      "encoder.layers.2.blocks.6.attention.self.value.weight\n",
      "encoder.layers.2.blocks.6.attention.self.value.bias\n",
      "encoder.layers.2.blocks.6.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.6.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.6.layernorm_after.weight\n",
      "encoder.layers.2.blocks.6.layernorm_after.bias\n",
      "encoder.layers.2.blocks.6.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.6.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.6.output.dense.weight\n",
      "encoder.layers.2.blocks.6.output.dense.bias\n",
      "encoder.layers.2.blocks.7.layernorm_before.weight\n",
      "encoder.layers.2.blocks.7.layernorm_before.bias\n",
      "encoder.layers.2.blocks.7.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.7.attention.self.query.weight\n",
      "encoder.layers.2.blocks.7.attention.self.query.bias\n",
      "encoder.layers.2.blocks.7.attention.self.key.weight\n",
      "encoder.layers.2.blocks.7.attention.self.key.bias\n",
      "encoder.layers.2.blocks.7.attention.self.value.weight\n",
      "encoder.layers.2.blocks.7.attention.self.value.bias\n",
      "encoder.layers.2.blocks.7.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.7.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.7.layernorm_after.weight\n",
      "encoder.layers.2.blocks.7.layernorm_after.bias\n",
      "encoder.layers.2.blocks.7.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.7.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.7.output.dense.weight\n",
      "encoder.layers.2.blocks.7.output.dense.bias\n",
      "encoder.layers.2.blocks.8.layernorm_before.weight\n",
      "encoder.layers.2.blocks.8.layernorm_before.bias\n",
      "encoder.layers.2.blocks.8.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.8.attention.self.query.weight\n",
      "encoder.layers.2.blocks.8.attention.self.query.bias\n",
      "encoder.layers.2.blocks.8.attention.self.key.weight\n",
      "encoder.layers.2.blocks.8.attention.self.key.bias\n",
      "encoder.layers.2.blocks.8.attention.self.value.weight\n",
      "encoder.layers.2.blocks.8.attention.self.value.bias\n",
      "encoder.layers.2.blocks.8.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.8.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.8.layernorm_after.weight\n",
      "encoder.layers.2.blocks.8.layernorm_after.bias\n",
      "encoder.layers.2.blocks.8.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.8.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.8.output.dense.weight\n",
      "encoder.layers.2.blocks.8.output.dense.bias\n",
      "encoder.layers.2.blocks.9.layernorm_before.weight\n",
      "encoder.layers.2.blocks.9.layernorm_before.bias\n",
      "encoder.layers.2.blocks.9.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.9.attention.self.query.weight\n",
      "encoder.layers.2.blocks.9.attention.self.query.bias\n",
      "encoder.layers.2.blocks.9.attention.self.key.weight\n",
      "encoder.layers.2.blocks.9.attention.self.key.bias\n",
      "encoder.layers.2.blocks.9.attention.self.value.weight\n",
      "encoder.layers.2.blocks.9.attention.self.value.bias\n",
      "encoder.layers.2.blocks.9.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.9.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.9.layernorm_after.weight\n",
      "encoder.layers.2.blocks.9.layernorm_after.bias\n",
      "encoder.layers.2.blocks.9.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.9.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.9.output.dense.weight\n",
      "encoder.layers.2.blocks.9.output.dense.bias\n",
      "encoder.layers.2.blocks.10.layernorm_before.weight\n",
      "encoder.layers.2.blocks.10.layernorm_before.bias\n",
      "encoder.layers.2.blocks.10.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.10.attention.self.query.weight\n",
      "encoder.layers.2.blocks.10.attention.self.query.bias\n",
      "encoder.layers.2.blocks.10.attention.self.key.weight\n",
      "encoder.layers.2.blocks.10.attention.self.key.bias\n",
      "encoder.layers.2.blocks.10.attention.self.value.weight\n",
      "encoder.layers.2.blocks.10.attention.self.value.bias\n",
      "encoder.layers.2.blocks.10.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.10.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.10.layernorm_after.weight\n",
      "encoder.layers.2.blocks.10.layernorm_after.bias\n",
      "encoder.layers.2.blocks.10.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.10.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.10.output.dense.weight\n",
      "encoder.layers.2.blocks.10.output.dense.bias\n",
      "encoder.layers.2.blocks.11.layernorm_before.weight\n",
      "encoder.layers.2.blocks.11.layernorm_before.bias\n",
      "encoder.layers.2.blocks.11.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.11.attention.self.query.weight\n",
      "encoder.layers.2.blocks.11.attention.self.query.bias\n",
      "encoder.layers.2.blocks.11.attention.self.key.weight\n",
      "encoder.layers.2.blocks.11.attention.self.key.bias\n",
      "encoder.layers.2.blocks.11.attention.self.value.weight\n",
      "encoder.layers.2.blocks.11.attention.self.value.bias\n",
      "encoder.layers.2.blocks.11.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.11.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.11.layernorm_after.weight\n",
      "encoder.layers.2.blocks.11.layernorm_after.bias\n",
      "encoder.layers.2.blocks.11.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.11.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.11.output.dense.weight\n",
      "encoder.layers.2.blocks.11.output.dense.bias\n",
      "encoder.layers.2.blocks.12.layernorm_before.weight\n",
      "encoder.layers.2.blocks.12.layernorm_before.bias\n",
      "encoder.layers.2.blocks.12.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.12.attention.self.query.weight\n",
      "encoder.layers.2.blocks.12.attention.self.query.bias\n",
      "encoder.layers.2.blocks.12.attention.self.key.weight\n",
      "encoder.layers.2.blocks.12.attention.self.key.bias\n",
      "encoder.layers.2.blocks.12.attention.self.value.weight\n",
      "encoder.layers.2.blocks.12.attention.self.value.bias\n",
      "encoder.layers.2.blocks.12.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.12.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.12.layernorm_after.weight\n",
      "encoder.layers.2.blocks.12.layernorm_after.bias\n",
      "encoder.layers.2.blocks.12.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.12.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.12.output.dense.weight\n",
      "encoder.layers.2.blocks.12.output.dense.bias\n",
      "encoder.layers.2.blocks.13.layernorm_before.weight\n",
      "encoder.layers.2.blocks.13.layernorm_before.bias\n",
      "encoder.layers.2.blocks.13.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.13.attention.self.query.weight\n",
      "encoder.layers.2.blocks.13.attention.self.query.bias\n",
      "encoder.layers.2.blocks.13.attention.self.key.weight\n",
      "encoder.layers.2.blocks.13.attention.self.key.bias\n",
      "encoder.layers.2.blocks.13.attention.self.value.weight\n",
      "encoder.layers.2.blocks.13.attention.self.value.bias\n",
      "encoder.layers.2.blocks.13.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.13.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.13.layernorm_after.weight\n",
      "encoder.layers.2.blocks.13.layernorm_after.bias\n",
      "encoder.layers.2.blocks.13.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.13.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.13.output.dense.weight\n",
      "encoder.layers.2.blocks.13.output.dense.bias\n",
      "encoder.layers.2.blocks.14.layernorm_before.weight\n",
      "encoder.layers.2.blocks.14.layernorm_before.bias\n",
      "encoder.layers.2.blocks.14.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.14.attention.self.query.weight\n",
      "encoder.layers.2.blocks.14.attention.self.query.bias\n",
      "encoder.layers.2.blocks.14.attention.self.key.weight\n",
      "encoder.layers.2.blocks.14.attention.self.key.bias\n",
      "encoder.layers.2.blocks.14.attention.self.value.weight\n",
      "encoder.layers.2.blocks.14.attention.self.value.bias\n",
      "encoder.layers.2.blocks.14.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.14.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.14.layernorm_after.weight\n",
      "encoder.layers.2.blocks.14.layernorm_after.bias\n",
      "encoder.layers.2.blocks.14.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.14.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.14.output.dense.weight\n",
      "encoder.layers.2.blocks.14.output.dense.bias\n",
      "encoder.layers.2.blocks.15.layernorm_before.weight\n",
      "encoder.layers.2.blocks.15.layernorm_before.bias\n",
      "encoder.layers.2.blocks.15.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.15.attention.self.query.weight\n",
      "encoder.layers.2.blocks.15.attention.self.query.bias\n",
      "encoder.layers.2.blocks.15.attention.self.key.weight\n",
      "encoder.layers.2.blocks.15.attention.self.key.bias\n",
      "encoder.layers.2.blocks.15.attention.self.value.weight\n",
      "encoder.layers.2.blocks.15.attention.self.value.bias\n",
      "encoder.layers.2.blocks.15.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.15.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.15.layernorm_after.weight\n",
      "encoder.layers.2.blocks.15.layernorm_after.bias\n",
      "encoder.layers.2.blocks.15.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.15.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.15.output.dense.weight\n",
      "encoder.layers.2.blocks.15.output.dense.bias\n",
      "encoder.layers.2.blocks.16.layernorm_before.weight\n",
      "encoder.layers.2.blocks.16.layernorm_before.bias\n",
      "encoder.layers.2.blocks.16.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.16.attention.self.query.weight\n",
      "encoder.layers.2.blocks.16.attention.self.query.bias\n",
      "encoder.layers.2.blocks.16.attention.self.key.weight\n",
      "encoder.layers.2.blocks.16.attention.self.key.bias\n",
      "encoder.layers.2.blocks.16.attention.self.value.weight\n",
      "encoder.layers.2.blocks.16.attention.self.value.bias\n",
      "encoder.layers.2.blocks.16.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.16.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.16.layernorm_after.weight\n",
      "encoder.layers.2.blocks.16.layernorm_after.bias\n",
      "encoder.layers.2.blocks.16.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.16.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.16.output.dense.weight\n",
      "encoder.layers.2.blocks.16.output.dense.bias\n",
      "encoder.layers.2.blocks.17.layernorm_before.weight\n",
      "encoder.layers.2.blocks.17.layernorm_before.bias\n",
      "encoder.layers.2.blocks.17.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.17.attention.self.query.weight\n",
      "encoder.layers.2.blocks.17.attention.self.query.bias\n",
      "encoder.layers.2.blocks.17.attention.self.key.weight\n",
      "encoder.layers.2.blocks.17.attention.self.key.bias\n",
      "encoder.layers.2.blocks.17.attention.self.value.weight\n",
      "encoder.layers.2.blocks.17.attention.self.value.bias\n",
      "encoder.layers.2.blocks.17.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.17.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.17.layernorm_after.weight\n",
      "encoder.layers.2.blocks.17.layernorm_after.bias\n",
      "encoder.layers.2.blocks.17.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.17.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.17.output.dense.weight\n",
      "encoder.layers.2.blocks.17.output.dense.bias\n",
      "encoder.layers.2.downsample.reduction.weight\n",
      "encoder.layers.2.downsample.norm.weight\n",
      "encoder.layers.2.downsample.norm.bias\n",
      "encoder.layers.3.blocks.0.layernorm_before.weight\n",
      "encoder.layers.3.blocks.0.layernorm_before.bias\n",
      "encoder.layers.3.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.3.blocks.0.attention.self.query.weight\n",
      "encoder.layers.3.blocks.0.attention.self.query.bias\n",
      "encoder.layers.3.blocks.0.attention.self.key.weight\n",
      "encoder.layers.3.blocks.0.attention.self.key.bias\n",
      "encoder.layers.3.blocks.0.attention.self.value.weight\n",
      "encoder.layers.3.blocks.0.attention.self.value.bias\n",
      "encoder.layers.3.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.3.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.3.blocks.0.layernorm_after.weight\n",
      "encoder.layers.3.blocks.0.layernorm_after.bias\n",
      "encoder.layers.3.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.3.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.3.blocks.0.output.dense.weight\n",
      "encoder.layers.3.blocks.0.output.dense.bias\n",
      "encoder.layers.3.blocks.1.layernorm_before.weight\n",
      "encoder.layers.3.blocks.1.layernorm_before.bias\n",
      "encoder.layers.3.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.3.blocks.1.attention.self.query.weight\n",
      "encoder.layers.3.blocks.1.attention.self.query.bias\n",
      "encoder.layers.3.blocks.1.attention.self.key.weight\n",
      "encoder.layers.3.blocks.1.attention.self.key.bias\n",
      "encoder.layers.3.blocks.1.attention.self.value.weight\n",
      "encoder.layers.3.blocks.1.attention.self.value.bias\n",
      "encoder.layers.3.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.3.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.3.blocks.1.layernorm_after.weight\n",
      "encoder.layers.3.blocks.1.layernorm_after.bias\n",
      "encoder.layers.3.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.3.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.3.blocks.1.output.dense.weight\n",
      "encoder.layers.3.blocks.1.output.dense.bias\n",
      "layernorm.weight\n",
      "layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"microsoft/swin-base-patch4-window7-224\"  # Example model name\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Accessing layers\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "aspis/swin-finetuned-food101 does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFAutoModel\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maspis/swin-finetuned-food101\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Example model name\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Accessing layers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/PlatePerfect/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:2799\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2795\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   2796\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupport for sharded checkpoints using safetensors is coming soon!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2797\u001b[0m     )\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, WEIGHTS_NAME, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhas_file_kwargs):\n\u001b[0;32m-> 2799\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2800\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but there is a file for PyTorch weights. Use `from_pt=True` to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2802\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m load this model from those weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2803\u001b[0m     )\n\u001b[1;32m   2804\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2806\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2807\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2808\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: aspis/swin-finetuned-food101 does not appear to have a file named tf_model.h5 but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "model_name = \"aspis/swin-finetuned-food101\"  # Example model name\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Accessing layers\n",
    "for layer in model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "#Preprocessing:\n",
    "'''Next, image augmentation techniques such as rotation, translation, and flipping can be applied to increase the diversity of the training data.\n",
    "This helps the model to generalize better and perform well on unseen images.'''\n",
    "\n",
    "# Assumptions:\n",
    "# - `df_images` is a DataFrame with image paths and IDs\n",
    "# - `df_ingredients` is a DataFrame with ingredients information and IDs\n",
    "# - `your_label_columns` are the columns representing the ingredients information in the DataFrame\n",
    "\n",
    "# Merge both DataFrames based on the image IDs\n",
    "merged_df = pd.merge(df_images, df_ingredients, on='image_id')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data generators for images\n",
    "'''train_datagen is used to generate data for training the model by reading images from a directory, performing transformations, and normalization.\n",
    "\n",
    "ImageDataGenerator is a class in TensorFlow/Keras that generates batches of augmented/normalized data from image data.\n",
    "It provides a flexible way to preprocess and augment images on-the-fly during training, without needing to pre-process and store all the images in memory.'''\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255) #used below in train_generator\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  #used below in test_generator\n",
    "\n",
    "\n",
    "'''The train_generator generates batches of augmented/normalized data from image data and labels in the form of a DataFrame.\n",
    "In the provided code, the train_generator is specifically responsible for generating training data for the model.\n",
    "It generates batches of data from the DataFrame train_df, which contains information about image paths and corresponding labels (ingredients in this case).'''\n",
    "\n",
    "#VARIABLES in train_generator:\n",
    "'''The \"image_path\" parameter in the flow_from_dataframe() method indicates which column of the DataFrame contains the paths to the images.\n",
    "When you provide this parameter, the ImageDataGenerator will use the paths specified in that column to load the images for processing.'''\n",
    "\n",
    "'''your_label_columns represents the column(s) in your DataFrame that contain the labels or target variables for your classification task.\n",
    "In the context of the provided code, it indicates the column(s) that contain information about the ingredients or classes associated with each image.'''\n",
    "\n",
    "'''batch_size is a hyperparameter that determines the number of samples processed by the model in each training iteration or batch during the training process.'''\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col=\"image_path\",\n",
    "    y_col=your_label_columns,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='raw'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col=\"image_path\",\n",
    "    y_col=your_label_columns,\n",
    "    target_size=(image_height, image_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='raw'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "'''num_channels typically refers to the number of channels or color channels in an image.\n",
    "In the context of the provided code, num_channels specifies the number of color channels for the input images. For example:\n",
    "For grayscale images, num_channels would be 1.\n",
    "For RGB (Red, Green, Blue) color images, num_channels would be 3, as there are three color channels representing the intensity of each color.'''\n",
    "\n",
    "#Modified for multi-label classification:\n",
    "\n",
    "'''Each node in the output layer must use the SIGMOID activation.\n",
    "This will predict a probability of class membership for the label, a value between 0 and 1.\n",
    "Finally, the model must be fit with the BINARY CROSS-ENTROPY loss function.\n",
    "https://machinelearningmastery.com/multi-label-classification-with-deep-learning/'''\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, num_channels), padding='same'), #32 kernels with size 3,3\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    '''This layer performs max pooling, which reduces the spatial dimensions of the feature maps while retaining the most important information.\n",
    "(2, 2) specifies the size of the pooling window, which determines how much the image is downsampled.'''\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    '''These layers further extract features from the input image.\n",
    "    Each subsequent convolutional layer learns more complex and abstract features from the previous layer's output.'''\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    '''his layer flattens the 2D feature maps into a 1D vector.\n",
    "    It prepares the data for input into the fully connected (dense) layers.'''\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')           #num_classes is = number of ingredients\n",
    "])\n",
    "\n",
    "'''Dense: These layers are fully connected layers that perform classification based on the features extracted by the convolutional layers.\n",
    "The first Dense layer has 64 units, meaning it has 64 neurons. It applies the ReLU activation function to introduce non-linearity.\n",
    "The second Dense layer has num_classes units, which corresponds to the number of output classes (ingredients).\n",
    "It uses the sigmoid activation function to output probabilities for each class, as specified.'''\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=test_generator)\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_height, image_width, num_channels)),   #padding is missing!\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')           #num_classes is = number of ingredients\n",
    "])\n",
    "\n",
    "\n",
    "# Another model from the internet:\n",
    "''' https://medium.com/@imdhawaltank/recipe-detection-of-food-image-using-deep-learning-65eb382aeb38#:~:text=Deep%20learning%20algorithms%2C%20particularly%20convolutional,along%20with%20their%20corresponding%20recipes.'''\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (7, 7), activation='relu', input_shape=(image_height, image_width, num_channels)),   #padding is missing!\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten()\n",
    "\n",
    "    layers.Dense(128, activation='relu')  # intermediate layer\n",
    "\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PlatePerfect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
